{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리\n",
    "\n",
    "ML 알고리즘은 데이터에 기반하여 어떤 데이터를 입력으로 가지느냐에 따라 결과도 크게 달라짐. 사이킷런의 ML 알고리즘을 적용하기 전에 데이터에 대해 미리 처리해야 할 기본 사항들이 있음.\n",
    "\n",
    "\n",
    "1. 결손값, 즉 NaN, Null 값은 허용되지 않음. 따라서 이러한 Null 값은 고정된 다른 값으로 변환해야 함. \n",
    "\n",
    "    -> Null값이 얼마 없으면 feature의 평균값으로 대체 가능, 하지만 대부분이라면 해당 feature는 drop하는 것이 나음. 해당 feature가 중요도가 높고 Null을 단순히 평균값으로 대체 시 예측 왜곡이 심할 수 있다면 정밀한 대체 값을 선정하는 것이 중요.\n",
    "    \n",
    "    \n",
    "2. 문자열 입력값은 사이킷런에서 허용하지 않음. 따라서 문자열을 인코딩하여 숫자형으로 변환 해야함. 문자열 feature는 일반적으로 카테고리형 feature와 텍스트형 feature를 의미. 카테고리형은 코드 값 표현이 더 이해하기 쉬움, 텍스트형의 경우 feature vectorization 등의 기법으로 벡터화하거나 불필요한 feature의 경우 삭제(e.g. 주민번호나 단순 문자열 ID)\n",
    "\n",
    "\n",
    "### 데이터 인코딩\n",
    "\n",
    "대표적인 인코딩 방식으로는 **레이블 인코딩(Label encoding)**과 **원-핫 인코딩(One Hot encoding)**이 있음.\n",
    "\n",
    "1. Label Encoding\n",
    "\n",
    ": Category Feature를 코드형 숫자 값으로 변환.\n",
    "(e.g. TV : 1, 냉장고 : 2, 전자레인지 : 3, 컴퓨터 : 4)\n",
    "\n",
    "이때 '01', '02'와 같은 코드 값 역시 문자열이므로 1, 2와 같은 숫자형 값으로 변환돼야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코딩 변환값 :  [0 1 4 5 3 3 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "items = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']\n",
    "\n",
    "# LabelEncoder를 객체로 생성한 후, fit()과 transform()으로 레이블 인코딩 수행\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "print('인코딩 변환값 : ', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코딩 클래스 :  ['TV' '냉장고' '믹서' '선풍기' '전자레인지' '컴퓨터']\n"
     ]
    }
   ],
   "source": [
    "print('인코딩 클래스 : ', encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inverse_transform()을 통해 인코딩 된 값을 다시 디코딩할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디코딩 원본 값 :  ['전자레인지' '컴퓨터' '믹서' 'TV' '냉장고' '냉장고' '선풍기' '선풍기']\n"
     ]
    }
   ],
   "source": [
    "print('디코딩 원본 값 : ', encoder.inverse_transform([4, 5, 2, 0, 1, 1, 3, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단점**\n",
    "일괄적인 숫자 값으로 변환되면서 몇몇 ML 알고리즘에는 예측 성능이 떨어지는 경우가 있음.\n",
    "이는 숫자 값의 크고 작음에 대한 특성이 작용하기 때문!\n",
    "\n",
    "즉, 냉장고가1, 믹서가 2로 변환되면 1보다 2가 더 큰 값이므로 특정 ML 알고리즘에서 가중치가 더 부여되거나 더 중요하게 인식할 가능성이 발생!\n",
    "\n",
    "이러한 특성 때문에 레이블 인코딩은 **선형회귀**와 같은 ML 알고리즘에는 적용하면 안됨. **트리 계열**의 알고리즘은 숫자의 이러한 특성을 반영하지 않으므로 문제 없음.\n",
    "\n",
    "원-핫 인코딩은 레이블 인코딩의 이러한 문제점을 해결하기 위한 인코딩 방식!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 원-핫 인코딩(One-Hot Encoding)\n",
    "\n",
    "원-핫 인코딩은 feature 값의 유형에 따라 새로운 feature를 추가해 고유 값에 해당하는 column에만 1을 표시하고 나머지 column에는 0을 표시하는 방식.\n",
    "\n",
    "즉, row 형태로 돼 있는 feature의 고유 값을 column 형태로 차원을 변환한 뒤, 고유 값에 해당하는 column에만 1을 표시하고 나머지 column에는 0을 표시\n",
    "\n",
    "**주의사항**\n",
    "1. OneHotEncoder로 변환하기 전 모든 문자열 값이 숫자형 값으로 변환돼야 함.\n",
    "2. 입력 값으로 2차원 데이터가 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원-핫 인코딩 데이터\n",
      "[[1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n",
      "원-핫 인코딩 데이터 차원\n",
      "(8, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "items = ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']\n",
    "\n",
    "# 먼저 숫자 값으로 변환을 위해 LabelEncoder로 변환\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(items)\n",
    "labels = encoder.transform(items)\n",
    "\n",
    "# 2차원 데이터로 변환\n",
    "labels = labels.reshape(-1, 1)\n",
    "\n",
    "# 원-핫 인코딩을 적용\n",
    "oh_encoder = OneHotEncoder()\n",
    "oh_encoder.fit(labels)\n",
    "oh_labels = oh_encoder.transform(labels)\n",
    "print('원-핫 인코딩 데이터')\n",
    "print(oh_labels.toarray())\n",
    "print('원-핫 인코딩 데이터 차원')\n",
    "print(oh_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_dummies()**\n",
    "\n",
    "pandas에는 원-핫 인코딩을 더 쉽게 지원하는 API가 있음!\n",
    "사이킷런과 달리 문자열 카테고리 값을 숫자 형으로 변환할 필요 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_TV</th>\n",
       "      <th>item_냉장고</th>\n",
       "      <th>item_믹서</th>\n",
       "      <th>item_선풍기</th>\n",
       "      <th>item_전자레인지</th>\n",
       "      <th>item_컴퓨터</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_TV  item_냉장고  item_믹서  item_선풍기  item_전자레인지  item_컴퓨터\n",
       "0        1         0        0         0           0         0\n",
       "1        0         1        0         0           0         0\n",
       "2        0         0        0         0           1         0\n",
       "3        0         0        0         0           0         1\n",
       "4        0         0        0         1           0         0\n",
       "5        0         0        0         1           0         0\n",
       "6        0         0        1         0           0         0\n",
       "7        0         0        1         0           0         0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'item' : ['TV', '냉장고', '전자레인지', '컴퓨터', '선풍기', '선풍기', '믹서', '믹서']})\n",
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피처 스케일링과 정규화\n",
    "\n",
    "**feature scailing**이란?\n",
    "- 서로 다른 변수의 값 범위를 일정한 수준으로 맞추는 작업\n",
    "- 대표적으로 표준화(Standardization)와 정규화(Normalization)\n",
    "\n",
    "**표준화(Standardization)**란?\n",
    "- 데이터의 feature 각각이 평균이 0이고 분산이 1인 가우시안 정규 분포를 가진 값으로 변환하는 것을 의미\n",
    "- 표준화를 통해 변환될 feature $x$의 새로운 i번째 데이터를 $ x_i $_$new$라고 한다면 이 값은 원래 값에서 feature $x$의 평균을 뺀 값을 feature $x$의 표준편차로 나눈 값으로 계산할 수 있음\n",
    "\n",
    "## $x_i$_$new$ $ = \\frac{x_i - mean(x)}{stdev(x)}$\n",
    "\n",
    "\n",
    "**정규화(Normalization)**란?\n",
    "- 서로 다른 feature의 크기를 통일하기 위해 크기를 변환해주는 개념\n",
    "- e.g) feature A는 거리를 나타내는 변수로 0~100km로 주어지고 feature B는 금액을 나타내는 속성으로 값이 0 ~ 100억으로 주어진다면 이 변수를 모두 동일한 크기 단위로 비교하기 위해 값을 모두 0 ~ 1로 변환하는 것.\n",
    "- 즉, 개별 데이터의 크기를 모두 똑같은 단위로 변경\n",
    "- 새로운 데이터 $ x_i $_$new$는 원래 값에서 feature $x$의 최솟값을 뺀 값을 feature $x$의 최댓값과 최솟값의 차이로 나눈 값으로 변환할 수 있음\n",
    "\n",
    "## $x_i$_$new$ $ = \\frac{x_i - min(x)}{max(x) - min(x)}$\n",
    "\n",
    "- 그러나 사이킷런의 전처리에서 제공하는 Normalizer 모듈은 일반적인 정규화와 차이가 있음(큰 개념은 같음!)\n",
    "- 선형대수에서의 정규화 개념이 적용됐으며, **개별 벡터의 크기를 맞추기 위해 변환하는 것**을 의미!\n",
    "- 즉, 개별 벡터를 모든 feature vectore의 크기로 나눠 줌.\n",
    "- 세 개의 feature $x, y, z$가 있다고 하면 새로운 데이터 $ x_i $_$new$는 원래 값에서 세 개의 feature의 $i$번째 feature 값에 해당하는 크기를 합한 값으로 나눠 줌\n",
    "\n",
    "## $x_i$_$new = \\frac{x_i}{ \\sqrt{x_i^2 + y_i^2 + z_i^2} }$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StandardScaler\n",
    "\n",
    "- 위에서 설명한 표준화를 쉽게 지원하기 위한 클래스\n",
    "- 개별 feature를 평균이 0이고, 분산이 1인 값으로 변환!\n",
    "- 이렇게 가우시안 정규 분포를 가질 수 있도록 변환하는 것은 몇몇 알고리즘에서 매우 유용\n",
    "- 특히 사이킷런에서 구현한 RBF 커널을 이용하는 SVM(Support Vector Machine)이나 선형 회귀(Linear Regression), 로지스틱 회귀(Logistic Regression)는 데이터가 가우시안 분포를 가지고 있다고 가정하고 구현됐기 때문에 사전에 표준화를 적용하는 것은 예측 성능 향상에 중요한 요소가 될 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature들의 평균 값\n",
      "sepal length (cm)    5.843333\n",
      "sepal width (cm)     3.057333\n",
      "petal length (cm)    3.758000\n",
      "petal width (cm)     1.199333\n",
      "dtype: float64\n",
      "\n",
      "feature들의 분산 값\n",
      "sepal length (cm)    0.685694\n",
      "sepal width (cm)     0.189979\n",
      "petal length (cm)    3.116278\n",
      "petal width (cm)     0.581006\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# 붓꽃 데이터 세트를 로딩하고 DataFrame으로 변환\n",
    "iris = load_iris()\n",
    "iris_data = iris.data\n",
    "iris_df = pd.DataFrame(data = iris_data, columns = iris.feature_names)\n",
    "\n",
    "print('feature들의 평균 값')\n",
    "print(iris_df.mean())\n",
    "print('\\nfeature들의 분산 값')\n",
    "print(iris_df.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature들의 평균 값\n",
      "sepal length (cm)   -1.690315e-15\n",
      "sepal width (cm)    -1.842970e-15\n",
      "petal length (cm)   -1.698641e-15\n",
      "petal width (cm)    -1.409243e-15\n",
      "dtype: float64\n",
      "\n",
      "feature들의 분산 값\n",
      "sepal length (cm)    1.006711\n",
      "sepal width (cm)     1.006711\n",
      "petal length (cm)    1.006711\n",
      "petal width (cm)     1.006711\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# StandardScaler 객체 생성\n",
    "scaler = StandardScaler()\n",
    "# StandardScaler로 데이터 세트 변환. fit()과 transform() 호출.\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "# transform() 시 스케일 변환된 데이터 세트가 NumPy ndarray로 반환돼 이를 DataFrame으로 변환\n",
    "iris_df_scaled = pd.DataFrame(data = iris_scaled, columns = iris.feature_names)\n",
    "print('feature들의 평균 값')\n",
    "print(iris_df_scaled.mean())\n",
    "print('\\nfeature들의 분산 값')\n",
    "print(iris_df_scaled.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 칼럼 값의 평균이 0에 아주 가까운 값으로, 그리고 분산은 1에 아주 가까운 값으로 변환됐음을 알 수 있음!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaler\n",
    "\n",
    "- 데이터값을 0과 1 사이의 범위 값으로 변환(음수 값이 있으면 -1에서 1값으로 변환)\n",
    "- 데이터의 분포가 가우시안 분포가 아닐 경우에 Min, Max, Scale을 적용해 볼 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature들의 최솟값\n",
      "sepal length (cm)    0.0\n",
      "sepal width (cm)     0.0\n",
      "petal length (cm)    0.0\n",
      "petal width (cm)     0.0\n",
      "dtype: float64\n",
      "\n",
      "feature들의 최댓값\n",
      "sepal length (cm)    1.0\n",
      "sepal width (cm)     1.0\n",
      "petal length (cm)    1.0\n",
      "petal width (cm)     1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# MinMaxScaler 객체 생성\n",
    "scaler = MinMaxScaler()\n",
    "# MinMaxScaler로 데이터 세트 변환. fit()과 transform() 호출.\n",
    "scaler.fit(iris_df)\n",
    "iris_scaled = scaler.transform(iris_df)\n",
    "\n",
    "# transform() 시 스케일 변환된 데이터 세트가 NumPy ndarray로 반환돼 이를 DataFrame으로 변환\n",
    "iris_df_scaled = pd.DataFrame(data = iris_scaled, columns = iris.feature_names)\n",
    "print('feature들의 최솟값')\n",
    "print(iris_df_scaled.min())\n",
    "print('\\nfeature들의 최댓값')\n",
    "print(iris_df_scaled.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이터와 테스트 데이터의 스케일링 변환 시 유의점 \n",
    "StandardScaler나 MinMaxScaler와 같은 Scaler 객체를 이용해 데이터의 스케일링 변환 시 fit(), transform(), fit_transform() 메소드를 이용. \n",
    "일반적으로 fit()은 데이터 변환을 위한 기준 정보 설정을 적용하며 transform()은 이렇게 설정된 정보를 이용해 데ㅣ터를 변환.\n",
    "fit_transform()은 fit()과 transform()을 한 번에 적용.\n",
    "\n",
    "그런데 fit()과 transform() 적용 시 주의가 필요!!\n",
    "Scaler 객체를 이용해 학습 데이터 세트로 fit()과 transform()을 적용하면 test dataset로는 다시 fit()을 수행하지 않고 학습 데이터 세트로 fit()을 수행한 결과를 이용해 transform() 변환을 적용해야 한다는 것.\n",
    "\n",
    "-> 즉 학습 데이터로 fit()이 적용된 스케일링 기준 정보를 그대로 테스트 데이터에 적용해야 하며, 그렇지 않고 테스트 데이터로 다시 새로운 스케일링 기준 정보를 만들게 되면 학습 데이터와 테스트 데이터의 스케일링 기준 정보가 서로 달라져 올바른 예측 결과를 도출하지 못할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# 학습 데이터는 0부터 10까지, 테스트 데이터는 0부터 5까지 값을 가지는 데이터 세트로 생성\n",
    "# Scaler 클래스의 fit(), transform()은 2차원 이상 데이터만 가능하므로 reshape(-1, 1)로 차원 변경\n",
    "train_array = np.arange(0, 11).reshape(-1, 1)\n",
    "test_array = np.arange(0, 6).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 데이터는 0부터 10까지의 값을 가지는데, 이 데이터에 MinMaxScaler 객체의 fit()을 적용하면 최솟값, 0, 최댓값 10이 설정되며 1/10 Scale이 적용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 train_array 데이터 :  [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "Scale된 train_array 데이터 :  [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n"
     ]
    }
   ],
   "source": [
    "# MinMaxScaler 객체에 별도의 feature_range 파라미터 값을 지정하지 않으면 0~1 값으로 변환\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# fit()하게 되면 train_array 데이터의 최솟값이 0, 최댓값이 10으로 설정.\n",
    "scaler.fit(train_array)\n",
    "\n",
    "# 1/10 scale로 train_array 데이터 변환. 원본 10 -> 1로 변환됨.\n",
    "train_scaled = scaler.transform(train_array)\n",
    "\n",
    "print('원본 train_array 데이터 : ', np.round(train_array.reshape(-1), 2))\n",
    "print('Scale된 train_array 데이터 : ', np.round(train_scaled.reshape(-1), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 test_array 데이터 :  [0 1 2 3 4 5]\n",
      "Scale된 test_array 데이터 :  [0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# MinMaxScaler에 test_array를 fit()하게 되면 원본 데이터의 최솟값이 0, 최댓값이 5로 설정됨\n",
    "scaler.fit(test_array)\n",
    "\n",
    "# 1/5 scale로 test_array 데이터 변환함. 원본 5 -> 1로 변환\n",
    "test_scaled = scaler.transform(test_array)\n",
    "\n",
    "# test_array의 scale 변환 출력\n",
    "print('원본 test_array 데이터 : ', np.round(test_array.reshape(-1), 2))\n",
    "print('Scale된 test_array 데이터 : ', np.round(test_array.reshape(-1), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력 결과를 확인해보면 학습 데이터와 테스트 데이터의 스케일링이 맞지 않음을 확인할 수 있음.\n",
    "이렇게 되면 학습 데이터와 테스트 데이터의 서로 다른 원본값이 동일한 값으로 변환되는 결과 초래.\n",
    "\n",
    "머신러닝 모델은 학습 데이터를 기반으로 학습되기 때문에 반드시 테스트 데이터는 학습 데이터의 스케일링 기준에 따라야 함.\n",
    "\n",
    "따라서 테스트 데이터에 다시 **fit()**을 적용해서는 안 되며 학습 데이터로 이미 fit()이 적용된 Scaler 객체를 이용해 **transform()**으로 변환해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 train_array 데이터 :  [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "Scale된 train_array 데이터 :  [0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1. ]\n",
      "\n",
      "원본 test_array 데이터 :  [0 1 2 3 4 5]\n",
      "Scale된 test_array 데이터 :  [0.  0.1 0.2 0.3 0.4 0.5]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_array)\n",
    "train_scaled = scaler.transform(train_array)\n",
    "print('원본 train_array 데이터 : ', np.round(train_array.reshape(-1), 2))\n",
    "print('Scale된 train_array 데이터 : ', np.round(train_scaled.reshape(-1), 2))\n",
    "\n",
    "\n",
    "# test_array에 Scale 변환을 할 때는 반드시 fit()을 호출하지 않고 transform()만으로 변환해야 함\n",
    "test_scaled = scaler.transform(test_array)\n",
    "print('\\n원본 test_array 데이터 : ', np.round(test_array.reshape(-1), 2))\n",
    "print('Scale된 test_array 데이터 : ', np.round(test_scaled.reshape(-1), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_transform()을 적용할 때도 테스트 데이터에서는 절대 사용해서는 안됨!!\n",
    "\n",
    "이렇게 주의사항이 있으므로 **학습과 테스트 데이터를 분리하기 전에 먼저 전체 데이터 세트에 스케일링을 적용한 뒤 학습과 테스트 데이터 세트로 분리하는 것**이 바람직\n",
    "\n",
    "\n",
    "**주의할 점 요약**\n",
    "1. 가능하다면 전체 데이터의 스케일링 변환 적용한 뒤 학습과 테스트 데이터로 분리\n",
    "2. 1이 여의치 않을 때면 테스트 데이터 변환 시에는 fit()이나 fit_transform()을 적용하지 않고 학습데이터로 이미 fit()된 Scaler 객체를 이용해 transform()으로 변환\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
